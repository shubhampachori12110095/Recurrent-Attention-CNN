{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_x(image):\n",
    "    return tf.transpose(set_y(image))\n",
    "\n",
    "\n",
    "def set_y(image):\n",
    "    [height, width, _] = image.get_shape().as_list()\n",
    "    rows = list(range(height))\n",
    "    rows = [tf.ones((width,)) * x for x in rows]\n",
    "    return tf.stack(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attention_region_param(conv_layer):\n",
    "    \"\"\"output the three region proposal parameters, ie, tx, ty, tl\"\"\"\n",
    "    anp_pool = tf.nn.max_pool(conv_layer, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                            padding=\"SAME\") # padding setting may not correct here\n",
    "                                            # need investigation\n",
    "    anp_pool_flat = tf.contrib.layers.flatten(anp_pool)\n",
    "    \n",
    "    # gaussian initializer is used here in the original work\n",
    "    get_abc1 = tf.layers.dense(inputs=anp_pool_flat, units=1024, \n",
    "                               activation=tf.nn.tanh,\n",
    "                              kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                               )\n",
    "    get_abc2 = tf.layers.dense(inputs=get_abc1, units=3,\n",
    "                              activation=tf.nn.sigmoid,\n",
    "                              kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                              )\n",
    "    # get448 = (0 +   448 * get_abc2)^1\n",
    "    # the original image size is 448\n",
    "    # get448 has three elements, tx, ty, tl\n",
    "    get448 = tf.multiply(get_abc2, image_size)\n",
    "    return get_448\n",
    "    \n",
    "def get_tx_ty_tl(param):\n",
    "    return param[:, 0], param[:, 1], param[:, 2]\n",
    "\n",
    "def get_corners(param):\n",
    "    tx, ty, tl = param[0], param[1], param[2]\n",
    "    tx_top_left = tf.maximum(tx - tl, 0.)\n",
    "    ty_top_left = tf.maximum(ty - tl, 0.)\n",
    "    tx_bottom_right = tf.minimum(tx + tl, 1.)\n",
    "    ty_bottom_right = tf.minimum(ty + tl, 1.)\n",
    "    return tx_top_left, ty_top_left, tx_bottom_right, ty_bottom_right\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "def get_attention_region(image, param, k=10):\n",
    "    \"\"\"get the attention region given image and tx, ty, tl\"\"\"\n",
    "    # get tx, ty, tl, I am not sure that tensor indexing is working\n",
    "    # this way, but I will use it for now\n",
    "    # also the order of tx, ty, tl in param may be different\n",
    "    tx, ty, tl = get_tx_ty_tl(param)\n",
    "    tx_top_left, ty_top_left, tx_bottom_right, ty_bottom_right = get_corners(param)\n",
    "    \n",
    "    [_, height, width, _] = image.get_shape().as_list()\n",
    "    tx_top_left = tf.map_fn(lambda x: tf.ones((height, width)) * x, tx_top_left)\n",
    "    ty_top_left = tf.map_fn(lambda x: tf.ones((height, width)) * x, ty_top_left)\n",
    "    tx_bottom_right = tf.map_fn(lambda x: tf.ones((height, width)) * x, tx_bottom_right)\n",
    "    ty_bottom_right = tf.map_fn(lambda x: tf.ones((height, width)) * x, ty_bottom_right)\n",
    "    \n",
    "    h = lambda x: tf.divide(1.0, tf.add(1.0, tf.exp(tf.multiply(x, -k))))\n",
    "    # construct the mask image, I will use the most simple and obvious \n",
    "    # implementation for now, which may not even be able to run on \n",
    "    # tensorflow\n",
    "    # attention_mask = tf.zeros_like(image)\n",
    "    # use layers to replace this\n",
    "    mask_x = tf.map_fn(set_x, image)\n",
    "    mask_y = tf.map_fn(set_y, image)\n",
    "    \n",
    "    mask_x1 = h(tf.subtract(mask_x, tx_top_left))\n",
    "    mask_x2 = h(tf.subtract(mask_x, tx_bottom_right))\n",
    "    \n",
    "    mask_y1 = h(tf.subtract(mask_y, ty_top_left))\n",
    "    mask_y2 = h(tf.subtract(mask_y, ty_bottom_right))\n",
    "    \n",
    "    mask_x = mask_x1 - mask_x2\n",
    "    mask_y = mask_y1 - mask_y2\n",
    "    attention_mask = tf.multiply(mask_x, mask_y)\n",
    "    attention_mask = tf.stack([attention_mask, attention_mask, attention_mask], axis=3)\n",
    "    \n",
    "    # element wise multiplication of the original image and attention_mask\n",
    "    return tf.multiply(image, attention_mask)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def crop_and_zoom(image_param):\n",
    "    image = image_param[0]\n",
    "    tx, ty, tl = image_param[1]\n",
    "    out_image_size = image_param[2]\n",
    "    tx_tl = tx - tl if (tx - tl) > 0 else 0\n",
    "    ty_tl = ty - tl if (ty - tl) > 0 else 0\n",
    "    tx_br = tx + tl if (tx + tl) < 1 else 1\n",
    "    ty_br = ty + tl if (ty + tl) < 1 else 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def attention_crop(images, params, out_image_size,name=None):\n",
    "    with tf.name_scope(name, \"AttentionCrop\", [images, params, out_image_size]) as scope:\n",
    "        images = tf.convert_to_tensor(images, name=\"images\")\n",
    "        params = tf.convert_to_tensor(params, name=\"params\")\n",
    "        crop_size = tf.convert_to_tensor([out_image_size, out_image_size], name=\"crop_size\")\n",
    "        # computation \n",
    "        boxes = tf.map_fn(get_corners, params)\n",
    "        box_ind = tf.scan(lambda a, _: a + 1, params, -1)\n",
    "        out_images = tf.image.crop_and_resize(images, boxes, box_ind, crop_size)\n",
    "        return out_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 448, 448, 3)\n"
     ]
    }
   ],
   "source": [
    "path = \"./th.jpg\"\n",
    "image = imread(path)\n",
    "image = imresize(image, (448, 448))\n",
    "image_copy = image.copy()\n",
    "#plt.imshow(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = np.concatenate((image, image), axis=0)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same number of elements. First structure: <dtype: 'float32'>, second structure: (<tf.Tensor 'AttentionCrop_4/map/while/Maximum:0' shape=() dtype=float32>, <tf.Tensor 'AttentionCrop_4/map/while/Maximum_1:0' shape=() dtype=float32>, <tf.Tensor 'AttentionCrop_4/map/while/Minimum:0' shape=() dtype=float32>, <tf.Tensor 'AttentionCrop_4/map/while/Minimum_1:0' shape=() dtype=float32>).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5442fb586523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mattention_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attention_region_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcroped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m350\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-42e2f77a2113>\u001b[0m in \u001b[0;36mattention_crop\u001b[0;34m(images, params, out_image_size, name)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mcrop_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_image_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_image_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"crop_size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mbox_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mout_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_and_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/apps/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/apps/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2768\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2770\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2771\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/apps/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2597\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2599\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2600\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2601\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/apps/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2547\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2549\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2550\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/apps/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, tas)\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0mpacked_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0mpacked_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m       \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m       \u001b[0mflat_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m       \u001b[0mtas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/apps/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[0;34m(nest1, nest2, check_types)\u001b[0m\n\u001b[1;32m    144\u001b[0m     raise ValueError(\"The two structures don't have the same number of \"\n\u001b[1;32m    145\u001b[0m                      \u001b[0;34m\"elements. First structure: %s, second structure: %s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                      % (nest1, nest2))\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0m_recursive_assert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The two structures don't have the same number of elements. First structure: <dtype: 'float32'>, second structure: (<tf.Tensor 'AttentionCrop_4/map/while/Maximum:0' shape=() dtype=float32>, <tf.Tensor 'AttentionCrop_4/map/while/Maximum_1:0' shape=() dtype=float32>, <tf.Tensor 'AttentionCrop_4/map/while/Minimum:0' shape=() dtype=float32>, <tf.Tensor 'AttentionCrop_4/map/while/Minimum_1:0' shape=() dtype=float32>)."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, \n",
    "                                         image_shape[0],\n",
    "                                         image_shape[1],\n",
    "                                         image_shape[2]))\n",
    "    return x\n",
    "\n",
    "image_input = neural_net_image_input([448, 448, 3])\n",
    "attention_params = get_attention_region_param(image_input)\n",
    "\n",
    "croped = attention_crop(image_input, attention_params, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "images = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    params = sess.run(corners, feed_dict = {image_input: image})\n",
    "    #ys = sess.run(ys, feed_dict={image_input: image})\n",
    "    #xs = sess.run(xs, feed_dict={image_input: image})\n",
    "    attention_region = sess.run(attention_region, feed_dict={image_input:image})\n",
    "    for i in range(attention_region.shape[0]):\n",
    "        images.append(np.squeeze(attention_region[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "The proposed recurrent attention CNN is optimized by two types of supervision, i.e., intra-scale classification loss and inter-scale pairwise ranking loss, for alternatively \n",
    "generating accurate region attention and learning more fine-grained features. Specifically, we minimize an objective function following a multi-task loss. The loss function for an image sample is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "L \\left( X \\right) = \\sum_{s=1}^3 \\left( L_{cls} \\left( Y^{\\left( s \\right)}, Y^* \\right) \\right) + \\sum_{s=1}^2 \\left( L_{rank} \\left( p_t^{\\left(s \\right)}, p_t^{\\left(  s+1 \\right)} \\right) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "$p_t^{\\left(s \\right)}$ from pairwise ranking loss $L_{rank}$ denotes the prediction probability on the correct category labels t. Specifically, the lanking loss is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_{rank} \\left( p_t^{\\left(s \\right)}, p_t^{\\left(s + 1 \\right)}  \\right) = max \\left( 0, p_t^{\\left(s \\right)}  - p_t^{\\left( s + 1\\right)} + margin \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "* Input images (at scale 1) and attended regions (at scale 2, 3) are resized to 448*448 and 224*224 pixels respectively in training, due to the smaller object size in the corse scale.\n",
    "\n",
    "* we find that k in Eqn.(6) and the margin in Eqn.(9) are robust to optimization, thun we empirically set k as 10 and margin as 0.05.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training strategy\n",
    "1. we initialize convolutional/classification layers in Figure 2 (b1 to b3 and c1 to c3) by the same pre-trained VGG network from ImageNet.\n",
    "2. we consider a square (represented by tx, tx, tl) with the half length of the side of the original image. The square is selected by searching regions in the original image, with the highest response value in the last convolutional layer (i.e., conv5_4 in VGG-19). We can further obtain a smaller square by aalyzing convolutional responses at the second scale in a similar way. These selected squares are used to pre-train APN to obtain parameters in Figure 2 by learning the transformation from convolutional feature maps to {tx, ty, tl}.\n",
    "\n",
    "3. we optimize the parameters in the above two steps in an alternative way. Specifically, we keep APN parameters unchanged, and optimize the softmax losses at three scales to converge. Then we fix parameters in convolutional/classification layers, and switch to ranking loss to optimize the two APNs. The learning process for the two parts is iterative, until the two types of losses no longer change. Besides, tl at each scale is constrained to be no less than one-third of the previous tl at coarse scale, to avoid the incompleteness of object structures when tl is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questions\n",
    "1. Does the two APN share parameters or not?\n",
    "2. If the two APN do not share parametrs, have they been trained as one in the pre-training process.\n",
    "3. Does \"we keep APN parameters unchanged, and optimize the softmax losses at three scales to converge.\" means that the classification layer for 3 scales are trained seperately, thus, error derivatives is not following through the APN network.\n",
    "4. Optimize the two APNs seperately or together, ie, does the errer derivatives following through.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}