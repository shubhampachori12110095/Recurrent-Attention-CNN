{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_x(image):\n",
    "    return tf.transpose(set_y(image))\n",
    "\n",
    "\n",
    "def set_y(image):\n",
    "    [height, width, _] = image.get_shape().as_list()\n",
    "    rows = list(range(height))\n",
    "    rows = [tf.ones((width,)) * x for x in rows]\n",
    "    return tf.stack(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attention_region_param(conv_layer, image_size=448.0):\n",
    "    \"\"\"output the three region proposal parameters, ie, tx, ty, tl\"\"\"\n",
    "    anp_pool = tf.nn.max_pool(conv_layer, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                            padding=\"SAME\") # padding setting may not correct here\n",
    "                                            # need investigation\n",
    "    anp_pool_flat = tf.contrib.layers.flatten(anp_pool)\n",
    "    \n",
    "    # gaussian initializer is used here in the original work\n",
    "    get_abc1 = tf.layers.dense(inputs=anp_pool_flat, units=1024, \n",
    "                               activation=tf.nn.tanh,\n",
    "                              kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                               )\n",
    "    get_abc2 = tf.layers.dense(inputs=get_abc1, units=3,\n",
    "                              activation=tf.nn.sigmoid,\n",
    "                              kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                              )\n",
    "    # get448 = (0 +   448 * get_abc2)^1\n",
    "    # the original image size is 448\n",
    "    # get448 has three elements, tx, ty, tl\n",
    "    get448 = tf.multiply(get_abc2, image_size)\n",
    "    return get448\n",
    "    \n",
    "def get_tx_ty_tl(param):\n",
    "    return param[:, 0], param[:, 1], param[:, 2]\n",
    "\n",
    "def get_corners(param):\n",
    "    tx, ty, tl = get_tx_ty_tl(param)\n",
    "    tx_top_left = tx - tl\n",
    "    ty_top_left = ty - tl\n",
    "    tx_bottom_right = tx + tl\n",
    "    ty_bottom_right = ty + tl\n",
    "    return tx_top_left, ty_top_left, tx_bottom_right, ty_bottom_right\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "def get_attention_region(image, param, k=10):\n",
    "    \"\"\"get the attention region given image and tx, ty, tl\"\"\"\n",
    "    # get tx, ty, tl, I am not sure that tensor indexing is working\n",
    "    # this way, but I will use it for now\n",
    "    # also the order of tx, ty, tl in param may be different\n",
    "    tx, ty, tl = get_tx_ty_tl(param)\n",
    "    tx_top_left, ty_top_left, tx_bottom_right, ty_bottom_right = get_corners(param)\n",
    "    \n",
    "    [_, height, width, _] = image.get_shape().as_list()\n",
    "    tx_top_left = tf.map_fn(lambda x: tf.ones((height, width)) * x, tx_top_left)\n",
    "    ty_top_left = tf.map_fn(lambda x: tf.ones((height, width)) * x, ty_top_left)\n",
    "    tx_bottom_right = tf.map_fn(lambda x: tf.ones((height, width)) * x, tx_bottom_right)\n",
    "    ty_bottom_right = tf.map_fn(lambda x: tf.ones((height, width)) * x, ty_bottom_right)\n",
    "    \n",
    "    h = lambda x: tf.divide(1.0, tf.add(1.0, tf.exp(tf.multiply(x, -k))))\n",
    "    # construct the mask image, I will use the most simple and obvious \n",
    "    # implementation for now, which may not even be able to run on \n",
    "    # tensorflow\n",
    "    # attention_mask = tf.zeros_like(image)\n",
    "    # use layers to replace this\n",
    "    mask_x = tf.map_fn(set_x, image)\n",
    "    mask_y = tf.map_fn(set_y, image)\n",
    "    \n",
    "    mask_x1 = h(tf.subtract(mask_x, tx_top_left))\n",
    "    mask_x2 = h(tf.subtract(mask_x, tx_bottom_right))\n",
    "    \n",
    "    mask_y1 = h(tf.subtract(mask_y, ty_top_left))\n",
    "    mask_y2 = h(tf.subtract(mask_y, ty_bottom_right))\n",
    "    \n",
    "    mask_x = mask_x1 - mask_x2\n",
    "    mask_y = mask_y1 - mask_y2\n",
    "    attention_mask = tf.multiply(mask_x, mask_y)\n",
    "    attention_mask = tf.stack([attention_mask, attention_mask, attention_mask], axis=3)\n",
    "    \n",
    "    # element wise multiplication of the original image and attention_mask\n",
    "    return tf.multiply(image, attention_mask)\n",
    "\n",
    "\n",
    "\n",
    "def zoom_attention_region(param, attention_image):\n",
    "    \"\"\"zoom the proposed attention regin to the original image size,\n",
    "    This implementation is not working of course, since one can not\n",
    "    assign values to Tensor indices.\n",
    "    \"\"\"\n",
    "    # image is zoomed to 224*224 according to experiment section in the paper\n",
    "    tx, ty, tl = get_tx_ty_tl(param)\n",
    "    tx_top_left, ty_top_left, tx_bottom_right, ty_bottom_right = get_corners(param)\n",
    "\n",
    "    [_, height, width, _] = attention_image.get_shape().as_list()\n",
    "    upsampling_factor = tf.divide(height / 2, tl)\n",
    "    \n",
    "    # construct the amplifid image\n",
    "    image = tf.zeros_like(attention_image)\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            for alpha in range(2):\n",
    "                for beta in range(2):\n",
    "                    m = y / upsampling_factor + alpha\n",
    "                    n = x / upsampling_factor + beta\n",
    "                    image[y, x] += attention_image[ty_top_left + m, tx_top_left + n] / 4\n",
    "                    # the original equation in the paper confuses me, so I just \n",
    "                    # use the mean of of nearest four points\n",
    "                    #image[y, x] += (tf.abs(1 - alpha - y / upsampling_factor) *\n",
    "                    #               tf.abs(1 - beta - x / upsampling_factor) *\n",
    "                    #               attention_image[ty_top_left + m, tx_top_left + n])\n",
    "    return image\n",
    "\n",
    "    # element-wise multiplication between the original image at coarse\n",
    "    # scales and an attention mask\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 448, 448, 3)\n"
     ]
    }
   ],
   "source": [
    "path = \"./th.jpg\"\n",
    "image = imread(path)\n",
    "image = imresize(image, (448, 448))\n",
    "image_copy = image.copy()\n",
    "#plt.imshow(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = np.concatenate((image, image), axis=0)\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of mask x is (?, 448, 448)\n",
      "type of tx_top_left is (?,)\n"
     ]
    }
   ],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, \n",
    "                                         image_shape[0],\n",
    "                                         image_shape[1],\n",
    "                                         image_shape[2]))\n",
    "    return x\n",
    "\n",
    "image_input = neural_net_image_input([448, 448, 3])\n",
    "attention_params = get_attention_region_param(image_input)\n",
    "tx, ty, tl = get_tx_ty_tl(attention_params)\n",
    "corners = get_corners(attention_params)\n",
    "attention_region = get_attention_region(image_input, attention_params)\n",
    "mask_y = tf.map_fn(set_y, image_input)\n",
    "mask_x = tf.map_fn(set_x, image_input)\n",
    "tx_top_left = tx - tl\n",
    "ty_top_left = ty - tl\n",
    "tx_bottom_right = tx + tl\n",
    "ty_bottom_right = ty + tl\n",
    "k = 1\n",
    "h = lambda x: tf.divide(1.0, tf.add(1.0, tf.exp(tf.multiply(x, -k))))\n",
    "print(\"type of mask x is {}\".format(mask_x.get_shape()))\n",
    "print(\"type of tx_top_left is {}\".format(tx_top_left.get_shape()))\n",
    "tx_top_left = tf.map_fn(lambda x: tf.ones((448, 448)) * x, tx_top_left)\n",
    "mask_x1 = mask_x - tx_top_left\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "images = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    params = sess.run(corners, feed_dict = {image_input: image})\n",
    "    #ys = sess.run(ys, feed_dict={image_input: image})\n",
    "    #xs = sess.run(xs, feed_dict={image_input: image})\n",
    "    attention_region = sess.run(attention_region, feed_dict={image_input:image})\n",
    "    for i in range(attention_region.shape[0]):\n",
    "        images.append(np.squeeze(attention_region[i]))\n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "The proposed recurrent attention CNN is optimized by two types of supervision, i.e., intra-scale classification loss and inter-scale pairwise ranking loss, for alternatively \n",
    "generating accurate region attention and learning more fine-grained features. Specifically, we minimize an objective function following a multi-task loss. The loss function for an image sample is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "L \\left( X \\right) = \\sum_{s=1}^3 \\left( L_{cls} \\left( Y^{\\left( s \\right)}, Y^* \\right) \\right) + \\sum_{s=1}^2 \\left( L_{rank} \\left( p_t^{\\left(s \\right)}, p_t^{\\left(  s+1 \\right)} \\right) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "$p_t^{\\left(s \\right)}$ from pairwise ranking loss $L_{rank}$ denotes the prediction probability on the correct category labels t. Specifically, the lanking loss is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "L_{rank} \\left( p_t^{\\left(s \\right)}, p_t^{\\left(s + 1 \\right)}  \\right) = max \\left( 0, p_t^{\\left(s \\right)}  - p_t^{\\left( s + 1\\right)} + margin \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "* Input images (at scale 1) and attended regions (at scale 2, 3) are resized to 448*448 and 224*224 pixels respectively in training, due to the smaller object size in the corse scale.\n",
    "\n",
    "* we find that k in Eqn.(6) and the margin in Eqn.(9) are robust to optimization, thun we empirically set k as 10 and margin as 0.05.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training strategy\n",
    "1. we initialize convolutional/classification layers in Figure 2 (b1 to b3 and c1 to c3) by the same pre-trained VGG network from ImageNet.\n",
    "2. we consider a square (represented by tx, tx, tl) with the half length of the side of the original image. The square is selected by searching regions in the original image, with the highest response value in the last convolutional layer (i.e., conv5_4 in VGG-19). We can further obtain a smaller square by aalyzing convolutional responses at the second scale in a similar way. These selected squares are used to pre-train APN to obtain parameters in Figure 2 by learning the transformation from convolutional feature maps to {tx, ty, tl}.\n",
    "\n",
    "3. we optimize the parameters in the above two steps in an alternative way. Specifically, we keep APN parameters unchanged, and optimize the softmax losses at three scales to converge. Then we fix parameters in convolutional/classification layers, and switch to ranking loss to optimize the two APNs. The learning process for the two parts is iterative, until the two types of losses no longer change. Besides, tl at each scale is constrained to be no less than one-third of the previous tl at coarse scale, to avoid the incompleteness of object structures when tl is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questions\n",
    "1. Does the two APN share parameters or not?\n",
    "2. If the two APN do not share parametrs, have they been trained as one in the pre-training process.\n",
    "3. Does \"we keep APN parameters unchanged, and optimize the softmax losses at three scales to converge.\" means that the classification layer for 3 scales are trained seperately, thus, error derivatives is not following through the APN network.\n",
    "4. Optimize the two APNs seperately or together, ie, does the errer derivatives following through.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
